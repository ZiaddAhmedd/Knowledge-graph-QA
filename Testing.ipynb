{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import regex as re\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import coreferee\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "\n",
    "def resolve_coreference(text):\n",
    "    doc = nlp(text)\n",
    "    doc_list = list(doc)\n",
    "    # doc._.coref_chains.print()\n",
    "    resolving_indecies = []\n",
    "    for _,item in enumerate(doc._.coref_chains):\n",
    "        resolving_indecies.extend(item)\n",
    "        \n",
    "    for word in resolving_indecies:\n",
    "        new_word = \"\"\n",
    "        for index in word:\n",
    "            if doc[index]._.coref_chains.resolve(doc[index]) is not None:\n",
    "                temp = []\n",
    "                for item in doc._.coref_chains.resolve(doc[index]):\n",
    "                    temp.append(str(item))\n",
    "                new_word = \", \".join(temp)\n",
    "            \n",
    "                doc_list[index] = new_word\n",
    "\n",
    "    final_doc = []\n",
    "    for item in doc_list:\n",
    "        final_doc.append(str(item))\n",
    "    return \" \".join(final_doc)\n",
    "\n",
    "def extract_subjects(sentence):\n",
    "    subjects = {}\n",
    "    verbIdx = 0\n",
    "    for token in sentence:\n",
    "        if token.pos_ == \"VERB\" or token.pos_ == \"AUX\":\n",
    "            verbIdx += 1\n",
    "            subjectFlag = False\n",
    "            verb = token\n",
    "            for child in token.children:\n",
    "                if child.dep_ in (\"nsubj\", \"csubj\"):\n",
    "                    subtree_tokens = [str(t) for t in child.subtree]\n",
    "                    subjects[token] = (\" \".join(subtree_tokens), verbIdx)\n",
    "                    subjectFlag = True\n",
    "                elif child.dep_ == \"nsubjpass\":\n",
    "                    for child in verb.children:\n",
    "                        if child.dep_ == \"agent\" and len(list(child.children)) > 0:\n",
    "                            subject = [str(t) for t in list(child.children)[0].subtree]\n",
    "                            subject = \" \".join(subject)\n",
    "                            break\n",
    "                        else:\n",
    "                            subject = \"Unknown\"\n",
    "                    subjects[verb] = (subject, verbIdx)\n",
    "                    subjectFlag = True\n",
    "            if not subjectFlag:  # didn't find a normal subject\n",
    "                if token.dep_ == \"relcl\":\n",
    "                    subject = str(token.head)\n",
    "                    subjects[token] = (subject, verbIdx)  # should get the subtree of the subject\n",
    "                elif token.dep_ in (\"advcl\", \"conj\"):\n",
    "                    verb = token.head\n",
    "                    \n",
    "                    if verb in subjects:\n",
    "                        subjects[token] = (subjects[verb][0], verbIdx)\n",
    "                    else:\n",
    "                        subjects[token] = (\"Unknown\", verbIdx)  # replace \"Unknown\" with a suitable default\n",
    "                elif token.dep_ == \"xcomp\":\n",
    "                    verb = token.head\n",
    "                    if verb in subjects:\n",
    "                        subjects[token] = (subjects[verb][0], verbIdx)\n",
    "                    else:\n",
    "                        subjects[token] = (\"Unknown\", verbIdx)\n",
    "                    for child in verb.subtree:\n",
    "                        if child.dep_ in (\"dobj\", \"dative\", \"pobj\"):\n",
    "                            subtree_tokens = [str(t) for t in child.subtree]\n",
    "                            subjects[token] = (\" \".join(subtree_tokens), verbIdx)\n",
    "                            break\n",
    "                else:\n",
    "                    subjects[token] = (\"Unknown\", verbIdx)\n",
    "                                        \n",
    "    # (subject, verbIdx, verb)\n",
    "    return [(v[0], k, v[1]) for k, v in subjects.items()]             \n",
    "         \n",
    "                            \n",
    "def extract_objects(sentence):\n",
    "    objects = []\n",
    "    verbIdx = 0\n",
    "    for token in sentence:\n",
    "        if token.pos_ == \"VERB\" or token.pos_ == \"AUX\":\n",
    "            verbIdx += 1\n",
    "            for child in token.children:\n",
    "                if child.dep_ in (\"dobj\", \"dative\", \"attr\", \"oprd\", \"acomp\",\"ccomp\", \"xcomp\", \"nsubjpass\"):\n",
    "                    subtree_tokens = [str(t) for t in child.subtree]\n",
    "                    objects.append((\" \".join(subtree_tokens), token, verbIdx))\n",
    "                    \n",
    "    return objects\n",
    "\n",
    "def extract_state(sentence):\n",
    "    states = []\n",
    "    verbIdx = 0\n",
    "    for token in sentence:\n",
    "        if token.pos_ ==\"VERB\" or token.pos_ == \"AUX\":\n",
    "            verbIdx += 1\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"prep\":\n",
    "                    subtree_tokens = [str(t) for t in child.subtree if t != child]\n",
    "                    states.append(((\" \".join(subtree_tokens), token, verbIdx)))\n",
    "    return states\n",
    "\n",
    "def extract_time(sentence):\n",
    "    times = {}\n",
    "    verbIdx = 0\n",
    "    for token in sentence:\n",
    "        if token.pos_ == \"VERB\" or token.pos_ == \"AUX\":\n",
    "            verbIdx += 1\n",
    "            for child in token.subtree:\n",
    "                if child.ent_type_ == \"DATE\" or child.ent_type_ == \"TIME\":\n",
    "                    times[child.text] = (token, verbIdx)\n",
    "                    \n",
    "    return [(k, v[0], v[1]) for k, v in times.items()]\n",
    "\n",
    "def extract_location(sentence):\n",
    "    locations = {}\n",
    "    verbIdx = 0\n",
    "    for token in sentence:\n",
    "        if token.pos_ == \"VERB\" or token.pos_ == \"AUX\":\n",
    "            verbIdx += 1\n",
    "            for child in token.subtree:\n",
    "                if child.ent_type_ in (\"GPE\", \"LOC\", \"FAC\"):\n",
    "                    locations[child.text] = (token, verbIdx)\n",
    "                    \n",
    "    return [(k, v[0], v[1]) for k, v in locations.items()]\n",
    "                    \n",
    "\n",
    "def extract_facts(sentence):\n",
    "    sentence = nlp(sentence)\n",
    "    states = extract_state(sentence)\n",
    "    subjects = extract_subjects(sentence)\n",
    "    objects = extract_objects(sentence)\n",
    "    times = extract_time(sentence)\n",
    "    locations = extract_location(sentence)\n",
    "    \n",
    "    facts = pd.DataFrame(columns=[\"Subject\", \"Relation\", \"verbIdx\", \"Objects\", \"States\", \"Times\", \"Locations\"])\n",
    "    \n",
    "    for subject in subjects: #(Aly, is, 1), (Ziad,is, 2) \n",
    "        currentSubject = subject[0]\n",
    "        verb = subject[1].lemma_\n",
    "        verbIdx = subject[2]\n",
    "        mask = (facts['Subject'] != currentSubject) | (facts['Relation'] != verb)\n",
    "        if mask.all():\n",
    "            new_row = pd.DataFrame([{\"Subject\": currentSubject, \"Relation\": verb, \"verbIdx\": verbIdx, \"Objects\": [], \"States\": [], \"Times\": [], \"Locations\": []}])\n",
    "            facts = pd.concat([facts, new_row], ignore_index=True)\n",
    "\n",
    "    for obj in objects: #(happy, is, 1), (good, is, 2)\n",
    "        currentObj = obj[0]\n",
    "        verb = obj[1].lemma_\n",
    "        verbIdx = obj[2]\n",
    "        mask = (facts['Relation'] == verb) & (facts['verbIdx'] == verbIdx)\n",
    "        if mask.any():\n",
    "            oldObjects = list(facts.loc[mask, \"Objects\"].values[0])\n",
    "            oldObjects.append(currentObj)\n",
    "            for idx in facts.loc[mask].index:\n",
    "                facts.at[idx, \"Objects\"] = oldObjects\n",
    "            \n",
    "    for state in states:\n",
    "        currentState = state[0]\n",
    "        verb = state[1].lemma_\n",
    "        verbIdx = state[2]\n",
    "        mask = (facts['Relation'] == verb) & (facts['verbIdx'] == verbIdx)\n",
    "        if mask.any():\n",
    "            oldStates = list(facts.loc[mask, \"States\"].values[0])\n",
    "            oldStates.append(currentState)\n",
    "            for idx in facts.loc[mask].index:\n",
    "                facts.at[idx, \"States\"] = oldStates\n",
    "            \n",
    "    for time in times:\n",
    "        currentTime = time[0]\n",
    "        verb = time[1].lemma_\n",
    "        verbIdx = time[2]\n",
    "        mask = (facts['Relation'] == verb) & (facts['verbIdx'] == verbIdx)\n",
    "        if mask.any():\n",
    "            oldTimes = list(facts.loc[mask, \"Times\"].values[0])\n",
    "            oldTimes.append(currentTime)\n",
    "            for idx in facts.loc[mask].index:\n",
    "                facts.at[idx, \"Times\"] = oldTimes\n",
    "            \n",
    "    for location in locations:\n",
    "        currentLocation = location[0]\n",
    "        verb = location[1].lemma_\n",
    "        verbIdx = location[2]\n",
    "        mask = (facts['Relation'] == verb) & (facts['verbIdx'] == verbIdx)\n",
    "        if mask.any():\n",
    "            oldLocations = list(facts.loc[mask, \"Locations\"].values[0])\n",
    "            oldLocations.append(currentLocation)\n",
    "            for idx in facts.loc[mask].index:\n",
    "                facts.at[idx, \"Locations\"] = oldLocations\n",
    "            \n",
    "    facts = facts.drop(columns=[\"verbIdx\"])\n",
    "    return facts\n",
    "        \n",
    "def preprocess_context(doc):\n",
    "    text = doc.strip()\n",
    "    text.replace(\".\", \",\")\n",
    "    resolved_text = resolve_coreference(text)\n",
    "    resolved_text = resolved_text.strip()\n",
    "    resolved_text = resolved_text.replace(\"  \", \" \").replace(\" ,\", \",\").replace(\" .\", \".\").replace(\"\\n\", \"\")\n",
    "    return resolved_text\n",
    "\n",
    "def join_sentences_facts(sentences):\n",
    "    all_facts = pd.DataFrame(columns=[\"Subject\", \"Relation\", \"Objects\", \"States\", \"Times\", \"Locations\"])\n",
    "    for sentence in sentences:\n",
    "        facts = extract_facts(sentence)\n",
    "        all_facts = pd.concat([all_facts, facts])\n",
    "    all_facts = all_facts.groupby([\"Subject\", \"Relation\"], as_index=False).agg({\n",
    "        \"Objects\": lambda x: [item for sublist in x for item in sublist],\n",
    "        \"States\": lambda x: [item for sublist in x for item in sublist],\n",
    "        \"Times\": lambda x: [item for sublist in x for item in sublist],\n",
    "        \"Locations\": lambda x: [item for sublist in x for item in sublist]\n",
    "    })\n",
    "    return all_facts\n",
    "\n",
    "def change_subject_relation(factsDF):\n",
    "    for index, row in factsDF.iterrows():\n",
    "        factsDF.loc[index, \"Subject\"] = [row['Subject']]\n",
    "        factsDF.loc[index, \"Relation\"] = [row['Relation']]\n",
    "    return factsDF\n",
    "\n",
    "def similarity(factRow, questionRow, column):\n",
    "    if len(factRow[column]) == 0 or len(questionRow[column]) == 0 or factRow[column] == [\"Unknown\"] or questionRow[column] == [\"Unknown\"]:\n",
    "        return 0\n",
    "    columnString = \" \".join(factRow[column])\n",
    "    questionString = \" \".join(questionRow[column])\n",
    "    embeddingFact = model.encode(columnString)\n",
    "    embeddingQuestion = model.encode(questionString)\n",
    "    return util.cos_sim(embeddingFact, embeddingQuestion)\n",
    "\n",
    "        \n",
    "def cost_function(factsDf, questionFact, excludeColumns=[]):\n",
    "    cost = 0\n",
    "    maxFactIdx = 0\n",
    "    columnNames = [\"Subject\",\"Relation\", \"Objects\", \"States\", \"Times\", \"Locations\"]\n",
    "    for column in excludeColumns:\n",
    "        columnNames.remove(column)\n",
    "    for factIdx, factRow in factsDf.iterrows():\n",
    "        currCost = 0\n",
    "        for _, questionRow in questionFact.iterrows():\n",
    "            if len(factRow[excludeColumns[0]]) == 0:\n",
    "                continue\n",
    "            for column in columnNames:\n",
    "                currCost += similarity(factRow, questionRow, column)\n",
    "        if currCost > cost:\n",
    "            cost = currCost\n",
    "            maxFactIdx = factIdx\n",
    "    return maxFactIdx, cost\n",
    "\n",
    "def process_question_context(question, doc):\n",
    "    splitted_question = question.split(\" \")\n",
    "    question_type = splitted_question[0].lower()\n",
    "    question_nlp = nlp(question)\n",
    "    if question_nlp[0].ent_type_ == \"DATE\":\n",
    "        question_type = \"when\"\n",
    "    resolved_doc = preprocess_context(doc)\n",
    "    cleaned_doc = nlp(resolved_doc)\n",
    "    sentences = [one_sentence.text.strip() for one_sentence in cleaned_doc.sents]\n",
    "    \n",
    "    questionDF = extract_facts(question)\n",
    "    factsDF = join_sentences_facts(sentences)\n",
    "    \n",
    "    newFactsDF = change_subject_relation(factsDF)\n",
    "    newQuestionDF = change_subject_relation(questionDF)\n",
    "    \n",
    "    return newFactsDF, newQuestionDF, question_type\n",
    "\n",
    "def get_answer(factsDF, questionDF, question_type):\n",
    "    correctIdx, _ = cost_function(factsDF, questionDF, excludeColumns=[excludesPerQuestionType[question_type]])\n",
    "    answer = factsDF.loc[correctIdx, excludesPerQuestionType[question_type]]\n",
    "    if answer == []:\n",
    "        answer = factsDF.loc[correctIdx, \"States\"]    \n",
    "    return \" \".join(answer)\n",
    "    \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "nlp.add_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe('coreferee')\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "excludesPerQuestionType = {\n",
    "    \"when\": \"Times\",\n",
    "    \"where\": \"Locations\",\n",
    "    \"who\": \"Subject\",\n",
    "    \"what\": \"Objects\",\n",
    "    \"how\": \"States\"\n",
    "}   \n",
    "    \n",
    "# doc = \"\"\"\n",
    "# Lionel Andrés \"Leo\" Messi was born in 24 June 1987 is an Argentine professional footballer plays as a forward for and captains both Major League Soccer club Inter Miami and the Argentina national team.\n",
    "# He played in Barcelona in 2010.\n",
    "# Widely regarded as one of the greatest players of all time, Messi has won a record eight Ballon d'Or awards, a record six European Golden Shoes, and was named the world's best player for a record eight times by FIFA.\n",
    "# Until 2021, he had spent his entire professional career with Barcelona, where he won a club-record 34 trophies, including ten La Liga titles, seven Copa del Rey titles, and the UEFA Champions League four times.\n",
    "# With his country, he won the 2021 Copa América and the 2022 FIFA World Cup. A prolific goalscorer and creative playmaker, Messi holds the records for most goals, hat-tricks, and assists in La Liga. He has the most international goals by a South American male. Messi has scored over 800 senior career goals for club and country, and the most goals for a single club.\n",
    "# \"\"\"\n",
    "# question = \"how did messi play?\"\n",
    "# factsDF, questionDF, question_type = process_question_context(question, doc)\n",
    "# answer = get_answer(factsDF, questionDF, question_type)\n",
    "\n",
    "# print(\"========================================================\")\n",
    "# print(\"Question: \", question)\n",
    "# print(\"Answer: \", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rajpurkar/squad\")\n",
    "# train = dataset['train']\n",
    "validation = dataset['validation']\n",
    "\n",
    "random.seed(42)\n",
    "import getopt\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import json \n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 917/10570 [02:56<17:43,  9.08it/s]  c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      "  9%|▉         | 998/10570 [03:01<06:54, 23.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 48, out of 203: 23.645320197044335%\n",
      "EM: 25, out of 203: 12.31527093596059%\n",
      "BLEU: 50.59556901173984, out of 203: 24.923925621546722%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1046/10570 [03:04<10:22, 15.31it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 10%|█         | 1061/10570 [03:05<09:11, 17.25it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      "c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 19%|█▉        | 1990/10570 [04:40<18:47,  7.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 79, out of 319: 24.764890282131663%\n",
      "EM: 40, out of 319: 12.539184952978056%\n",
      "BLEU: 78.71942414422021, out of 319: 24.67693546840759%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2483/10570 [05:22<06:43, 20.02it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 28%|██▊       | 2992/10570 [06:00<04:02, 31.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 108, out of 396: 27.272727272727273%\n",
      "EM: 56, out of 396: 14.141414141414142%\n",
      "BLEU: 104.7792733956671, out of 396: 26.459412473653305%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3999/10570 [07:08<13:55,  7.87it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 131, out of 470: 27.872340425531913%\n",
      "EM: 62, out of 470: 13.191489361702128%\n",
      "BLEU: 119.30639816845809, out of 470: 25.384340035842147%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4926/10570 [08:03<07:54, 11.91it/s]  c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 47%|████▋     | 4938/10570 [08:03<06:57, 13.50it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 47%|████▋     | 4980/10570 [08:09<08:42, 10.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 153, out of 519: 29.479768786127167%\n",
      "EM: 68, out of 519: 13.102119460500964%\n",
      "BLEU: 135.03689774926175, out of 519: 26.01867008656296%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 6000/10570 [10:04<08:40,  8.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 171, out of 619: 27.62520193861066%\n",
      "EM: 79, out of 619: 12.762520193861066%\n",
      "BLEU: 154.8708122870352, out of 619: 25.01951733231586%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 6117/10570 [10:14<11:52,  6.25it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 58%|█████▊    | 6119/10570 [10:15<13:12,  5.62it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 59%|█████▊    | 6184/10570 [10:35<26:26,  2.76it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 59%|█████▉    | 6228/10570 [10:49<24:56,  2.90it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 60%|█████▉    | 6291/10570 [11:03<18:08,  3.93it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 60%|█████▉    | 6292/10570 [11:04<22:27,  3.18it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 66%|██████▌   | 6997/10570 [12:08<29:56,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 194, out of 712: 27.247191011235955%\n",
      "EM: 96, out of 712: 13.48314606741573%\n",
      "BLEU: 182.02166040716153, out of 712: 25.56483994482606%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 7232/10570 [12:12<01:04, 51.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title:  Harvard_University\n",
      "Error in question number:  718\n",
      "Question:  Who is the Costa Rican President that went to Harvard?\n",
      "Answer:  José María Figueres\n",
      "Error:  0\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 7999/10570 [13:34<09:09,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 221, out of 802: 27.556109725685786%\n",
      "EM: 110, out of 802: 13.71571072319202%\n",
      "BLEU: 203.96608911324253, out of 802: 25.432180687436723%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 8132/10570 [13:43<05:57,  6.82it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 77%|███████▋  | 8143/10570 [13:45<07:21,  5.49it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 77%|███████▋  | 8181/10570 [13:55<10:25,  3.82it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 82%|████████▏ | 8644/10570 [14:53<03:58,  8.09it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 85%|████████▌ | 8993/10570 [15:06<00:17, 88.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 243, out of 911: 26.67398463227223%\n",
      "EM: 120, out of 911: 13.172338090010976%\n",
      "BLEU: 225.508760416421, out of 911: 24.753980287203188%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 9676/10570 [16:04<01:20, 11.16it/s] c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 92%|█████████▏| 9699/10570 [16:06<01:06, 13.18it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 92%|█████████▏| 9706/10570 [16:06<01:05, 13.28it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 95%|█████████▍| 9999/10570 [16:47<02:18,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 271, out of 998: 27.15430861723447%\n",
      "EM: 137, out of 998: 13.72745490981964%\n",
      "BLEU: 252.76438396360533, out of 998: 25.32709258152358%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10570/10570 [17:56<00:00,  9.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 294, out of 1059: 27.762039660056658%\n",
      "EM: 151, out of 1059: 14.258734655335221%\n",
      "BLEU: 273.20585499633404, out of 1059: 25.798475448190185%\n",
      "Errors:  [718]\n",
      "Empties:  0 []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def QuestionStartsWith_Accuracy(dataset, startsWith):\n",
    "\n",
    "    correct = 0\n",
    "    EM = 0\n",
    "    BLEU = 0\n",
    "    total = 0\n",
    "    errors = []\n",
    "    corrects = []\n",
    "    empties = []\n",
    "    kolo = 0\n",
    "    for item in tqdm(dataset):\n",
    "        random_number = random.randint(0, len(dataset))\n",
    "        kolo += 1\n",
    "        included_question = False\n",
    "        try:\n",
    "            context = item['context']\n",
    "            context = re.sub(' +', ' ', context)\n",
    "            \n",
    "            question = item['question']\n",
    "            tempQuestion = question.lower()\n",
    "            question = re.sub(' +', ' ', question)\n",
    "            \n",
    "            answer = item['answers']['text'][0]\n",
    "            title = item['title']\n",
    "    \n",
    "            # check if question starts with startsWith\n",
    "            for start in startsWith:\n",
    "                if tempQuestion.startswith(start):\n",
    "                    included_question = True\n",
    "                    break\n",
    "            \n",
    "            if included_question:\n",
    "    \n",
    "                total += 1\n",
    "                if total == 74:\n",
    "                    pass\n",
    "                    \n",
    "                factsDF, questionDF, question_type = process_question_context(question, context)\n",
    "                outputAnswer = get_answer(factsDF, questionDF, question_type)\n",
    "                if outputAnswer == \"\":\n",
    "                    empties.append(kolo)\n",
    "                    outputAnswer = \"No_Answer_Found\"\n",
    "                \n",
    "                # print(\"Question: \" , question)\n",
    "                # print(\"Answer: \", answer, \"-------\" , \"Our Answer: \", outputAnswer)\n",
    "                if outputAnswer in answer or answer in outputAnswer:\n",
    "                    correct += 1\n",
    "                    corrects.append(kolo)\n",
    "                # else:\n",
    "                #     print(\"Question: \" , question)\n",
    "                #     print(\"Answer: \", answer, \"-------\" , \"Our Answer: \", outputAnswer)\n",
    "                if outputAnswer == answer:\n",
    "                    EM += 1\n",
    "\n",
    "                n = min(len(outputAnswer.split()), 4)\n",
    "                if n == 0:\n",
    "                    BLEUscore = 0\n",
    "                else:\n",
    "                    weights = [1.0/n]*n\n",
    "                    smoothie = SmoothingFunction().method4\n",
    "                    BLEUscore = nltk.translate.bleu_score.sentence_bleu([answer], outputAnswer, weights=weights, smoothing_function=smoothie)\n",
    "                    BLEU += BLEUscore\n",
    "                    \n",
    "\n",
    "            if kolo % 1000 == 0:\n",
    "                print(f\"Correct: {correct}, out of {total}: {100*correct/total}%\")\n",
    "                print(f\"EM: {EM}, out of {total}: {100*EM/total}%\")\n",
    "                print(f\"BLEU: {BLEU}, out of {total}: {100*BLEU/total}%\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"title: \", title)\n",
    "            print(\"Error in question number: \", total)\n",
    "            print(\"Question: \", question)\n",
    "            print(\"Answer: \", answer)\n",
    "            print(\"Error: \", e)\n",
    "            errors.append(total)\n",
    "            # total -= 1\n",
    "            print(\"\\n\\n\")\n",
    "    \n",
    "    if total != 0:\n",
    "        print(f\"Correct: {correct}, out of {total}: {100*correct/total}%\")\n",
    "        print(f\"EM: {EM}, out of {total}: {100*EM/total}%\")\n",
    "        print(f\"BLEU: {BLEU}, out of {total}: {100*BLEU/total}%\")\n",
    "    else:\n",
    "        print(\"No Questions found with the given starting word\")\n",
    "    print(\"Errors: \", errors)\n",
    "    print(\"Empties: \", len(empties), empties)\n",
    "    return correct, total, errors, corrects\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    startsWith = [\"who \"]\n",
    "    x = QuestionStartsWith_Accuracy(validation, startsWith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [174, 315, 326, 349, 472, 488, 493, 498, 530, 667]\n",
    "# when: 48%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
