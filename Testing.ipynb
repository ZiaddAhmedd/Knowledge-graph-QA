{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import regex as re\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import coreferee\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "\n",
    "def resolve_coreference(text):\n",
    "    doc = nlp(text)\n",
    "    doc_list = list(doc)\n",
    "    # doc._.coref_chains.print()\n",
    "    resolving_indecies = []\n",
    "    for _,item in enumerate(doc._.coref_chains):\n",
    "        resolving_indecies.extend(item)\n",
    "        \n",
    "    for word in resolving_indecies:\n",
    "        new_word = \"\"\n",
    "        for index in word:\n",
    "            if doc[index]._.coref_chains.resolve(doc[index]) is not None:\n",
    "                temp = []\n",
    "                for item in doc._.coref_chains.resolve(doc[index]):\n",
    "                    temp.append(str(item))\n",
    "                new_word = \", \".join(temp)\n",
    "            \n",
    "                doc_list[index] = new_word\n",
    "\n",
    "    final_doc = []\n",
    "    for item in doc_list:\n",
    "        final_doc.append(str(item))\n",
    "    return \" \".join(final_doc)\n",
    "\n",
    "def extract_subjects(sentence):\n",
    "    subjects = {}\n",
    "    verbIdx = 0\n",
    "    for token in sentence:\n",
    "        if token.pos_ == \"VERB\" or token.pos_ == \"AUX\" or token.dep_ == \"ROOT\":\n",
    "            verbIdx += 1\n",
    "            subjectFlag = False\n",
    "            verb = token\n",
    "            for child in token.children:\n",
    "                if child.dep_ in (\"nsubj\", \"csubj\"):\n",
    "                    subtree_tokens = [str(t) for t in child.subtree]\n",
    "                    subjects[token] = (\" \".join(subtree_tokens), verbIdx)\n",
    "                    subjectFlag = True\n",
    "                elif child.dep_ == \"nsubjpass\":\n",
    "                    for child in verb.children:\n",
    "                        if child.dep_ == \"agent\" and len(list(child.children)) > 0:\n",
    "                            subject = [str(t) for t in list(child.children)[0].subtree]\n",
    "                            subject = \" \".join(subject)\n",
    "                            break\n",
    "                        else:\n",
    "                            subject = \"Unknown\"\n",
    "                    subjects[verb] = (subject, verbIdx)\n",
    "                    subjectFlag = True\n",
    "            if not subjectFlag:  # didn't find a normal subject\n",
    "                if token.dep_ in (\"relcl\" , \"acl\"):\n",
    "                    subject = str(token.head)\n",
    "                    subjects[token] = (subject, verbIdx)  # should get the subtree of the subject\n",
    "                elif token.dep_ in (\"advcl\", \"conj\"):\n",
    "                    verb = token.head\n",
    "                    if verb in subjects:\n",
    "                        subjects[token] = (subjects[verb][0], verbIdx)\n",
    "                    else:\n",
    "                        subjects[token] = (\"Unknown\", verbIdx)  # replace \"Unknown\" with a suitable default\n",
    "                elif token.dep_ == \"xcomp\":\n",
    "                    verb = token.head\n",
    "                    if verb in subjects:\n",
    "                        subjects[token] = (subjects[verb][0], verbIdx)\n",
    "                    else:\n",
    "                        subjects[token] = (\"Unknown\", verbIdx)\n",
    "                    for child in verb.subtree:\n",
    "                        if child.dep_ in (\"dobj\", \"dative\", \"pobj\"):\n",
    "                            subtree_tokens = [str(t) for t in child.subtree]\n",
    "                            subjects[token] = (\" \".join(subtree_tokens), verbIdx)\n",
    "                            break\n",
    "                else:\n",
    "                    subjects[token] = (\"Unknown\", verbIdx)\n",
    "                                        \n",
    "    # (subject, verbIdx, verb)\n",
    "    return [(v[0], k, v[1]) for k, v in subjects.items()]             \n",
    "                            \n",
    "def extract_objects(sentence):\n",
    "    objects = []\n",
    "    verbIdx = 0\n",
    "    for token in sentence:\n",
    "        if token.pos_ == \"VERB\" or token.pos_ == \"AUX\" or token.dep_ == \"ROOT\":\n",
    "            verbIdx += 1\n",
    "            for child in token.children:\n",
    "                if child.dep_ in (\"dobj\", \"dative\", \"attr\", \"oprd\", \"acomp\",\"ccomp\", \"xcomp\", \"nsubjpass\"):\n",
    "                    subtree_tokens = [str(t) for t in child.subtree]\n",
    "                    objects.append((\" \".join(subtree_tokens), token, verbIdx))\n",
    "                    \n",
    "    return objects\n",
    "\n",
    "def extract_state(sentence):\n",
    "    states = []\n",
    "    verbIdx = 0\n",
    "    for token in sentence:\n",
    "        if token.pos_ ==\"VERB\" or token.pos_ == \"AUX\":\n",
    "            verbIdx += 1\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"prep\":\n",
    "                    subtree_tokens = [str(t) for t in child.subtree]\n",
    "                    states.append(((\" \".join(subtree_tokens), token, verbIdx)))\n",
    "    return states\n",
    "\n",
    "def extract_time(sentence):\n",
    "    times = {}\n",
    "    verbIdx = 0\n",
    "    year_pattern = re.compile(r'\\b\\d{4}\\b')  # matches any four-digit number\n",
    "    for token in sentence:\n",
    "        if token.pos_ == \"VERB\" or token.pos_ == \"AUX\" or token.dep_ == \"ROOT\":\n",
    "            verbIdx += 1\n",
    "            for child in token.subtree:\n",
    "                if child.ent_type_ == \"DATE\" or child.ent_type_ == \"TIME\":\n",
    "                    times[child.text] = (token, verbIdx)\n",
    "                elif year_pattern.search(child.text):\n",
    "                    year = year_pattern.search(child.text).group()\n",
    "                    times[year] = (token, verbIdx)\n",
    "    return [(k, v[0], v[1]) for k, v in times.items()]\n",
    "\n",
    "def extract_location(sentence):\n",
    "    locations = {}\n",
    "    verbIdx = 0\n",
    "    for token in sentence:\n",
    "        if token.pos_ == \"VERB\" or token.pos_ == \"AUX\" or token.dep_ == \"ROOT\":\n",
    "            verbIdx += 1\n",
    "            for child in token.subtree:\n",
    "                if child.ent_type_ in (\"GPE\", \"LOC\", \"FAC\"):\n",
    "                    locations[child.text] = (token, verbIdx)\n",
    "                    \n",
    "    return [(k, v[0], v[1]) for k, v in locations.items()]\n",
    "                                        \n",
    "\n",
    "def extract_facts(sentence):\n",
    "    sentence = nlp(sentence)\n",
    "    states = extract_state(sentence)\n",
    "    subjects = extract_subjects(sentence)\n",
    "    objects = extract_objects(sentence)\n",
    "    times = extract_time(sentence)\n",
    "    locations = extract_location(sentence)\n",
    "    \n",
    "    facts = pd.DataFrame(columns=[\"Subject\", \"Relation\", \"verbIdx\", \"Objects\", \"States\", \"Times\", \"Locations\"])\n",
    "    \n",
    "    for subject in subjects: #(Aly, is, 1), (Ziad,is, 2) \n",
    "        currentSubject = subject[0]\n",
    "        verb = subject[1].lemma_\n",
    "        verbIdx = subject[2]\n",
    "        mask = (facts['Subject'] != currentSubject) | (facts['Relation'] != verb)\n",
    "        if mask.all():\n",
    "            new_row = pd.DataFrame([{\"Subject\": currentSubject, \"Relation\": verb, \"verbIdx\": verbIdx, \"Objects\": [], \"States\": [], \"Times\": [], \"Locations\": []}])\n",
    "            facts = pd.concat([facts, new_row], ignore_index=True)\n",
    "\n",
    "    for obj in objects: #(happy, is, 1), (good, is, 2)\n",
    "        currentObj = obj[0]\n",
    "        verb = obj[1].lemma_\n",
    "        verbIdx = obj[2]\n",
    "        mask = (facts['Relation'] == verb) & (facts['verbIdx'] == verbIdx)\n",
    "        if mask.any():\n",
    "            oldObjects = list(facts.loc[mask, \"Objects\"].values[0])\n",
    "            oldObjects.append(currentObj)\n",
    "            for idx in facts.loc[mask].index:\n",
    "                facts.at[idx, \"Objects\"] = oldObjects\n",
    "            \n",
    "    for state in states:\n",
    "        currentState = state[0]\n",
    "        verb = state[1].lemma_\n",
    "        verbIdx = state[2]\n",
    "        mask = (facts['Relation'] == verb) & (facts['verbIdx'] == verbIdx)\n",
    "        if mask.any():\n",
    "            oldStates = list(facts.loc[mask, \"States\"].values[0])\n",
    "            oldStates.append(currentState)\n",
    "            for idx in facts.loc[mask].index:\n",
    "                facts.at[idx, \"States\"] = oldStates\n",
    "            \n",
    "    for time in times:\n",
    "        currentTime = time[0]\n",
    "        verb = time[1].lemma_\n",
    "        verbIdx = time[2]\n",
    "        mask = (facts['Relation'] == verb) & (facts['verbIdx'] == verbIdx)\n",
    "        if mask.any():\n",
    "            oldTimes = list(facts.loc[mask, \"Times\"].values[0])\n",
    "            oldTimes.append(currentTime)\n",
    "            for idx in facts.loc[mask].index:\n",
    "                facts.at[idx, \"Times\"] = oldTimes\n",
    "            \n",
    "    for location in locations:\n",
    "        currentLocation = location[0]\n",
    "        verb = location[1].lemma_\n",
    "        verbIdx = location[2]\n",
    "        mask = (facts['Relation'] == verb) & (facts['verbIdx'] == verbIdx)\n",
    "        if mask.any():\n",
    "            oldLocations = list(facts.loc[mask, \"Locations\"].values[0])\n",
    "            oldLocations.append(currentLocation)\n",
    "            for idx in facts.loc[mask].index:\n",
    "                facts.at[idx, \"Locations\"] = oldLocations\n",
    "            \n",
    "    facts = facts.drop(columns=[\"verbIdx\"])\n",
    "    return facts\n",
    "        \n",
    "def preprocess_context(doc):\n",
    "    text = doc.strip()\n",
    "    text.replace(\".\", \",\")\n",
    "    resolved_text = resolve_coreference(text)\n",
    "    resolved_text = resolved_text.strip()\n",
    "    resolved_text = resolved_text.replace(\"  \", \" \").replace(\" ,\", \",\").replace(\" .\", \".\").replace(\"\\n\", \"\")\n",
    "    return resolved_text\n",
    "\n",
    "def join_sentences_facts(sentences):\n",
    "    all_facts = pd.DataFrame(columns=[\"Subject\", \"Relation\", \"Objects\", \"States\", \"Times\", \"Locations\"])\n",
    "    for sentence in sentences:\n",
    "        facts = extract_facts(sentence)\n",
    "        all_facts = pd.concat([all_facts, facts])\n",
    "    all_facts = all_facts.groupby([\"Subject\", \"Relation\"], as_index=False).agg({\n",
    "        \"Objects\": lambda x: [item for sublist in x for item in sublist],\n",
    "        \"States\": lambda x: [item for sublist in x for item in sublist],\n",
    "        \"Times\": lambda x: [item for sublist in x for item in sublist],\n",
    "        \"Locations\": lambda x: [item for sublist in x for item in sublist]\n",
    "    })\n",
    "    return all_facts\n",
    "\n",
    "def change_subject_relation(factsDF, isQuestion = True):\n",
    "    if not isQuestion:\n",
    "        factsDF = factsDF[~((factsDF[\"Subject\"] == \"Unknown\") & (factsDF[\"Objects\"].apply(len) == 0) & (factsDF[\"States\"].apply(len) == 0) & (factsDF[\"Times\"].apply(len) == 0) & (factsDF[\"Locations\"].apply(len) == 0))]\n",
    "        factsDF = factsDF.reset_index(drop=True)\n",
    "\n",
    "    for index, row in factsDF.iterrows():\n",
    "        factsDF.loc[index, \"Subject\"] = [row['Subject']]\n",
    "        factsDF.loc[index, \"Relation\"] = [row['Relation']]\n",
    "    return factsDF\n",
    "\n",
    "def similarity(factRow, questionRow, column):\n",
    "    if len(factRow[column]) == 0 or len(questionRow[column]) == 0 or factRow[column] == [\"Unknown\"] or questionRow[column] == [\"Unknown\"]:\n",
    "        return 0\n",
    "    columnString = \" \".join(factRow[column])\n",
    "    questionString = \" \".join(questionRow[column])\n",
    "    embeddingFact = model.encode(columnString)\n",
    "    embeddingQuestion = model.encode(questionString)\n",
    "    return util.cos_sim(embeddingFact, embeddingQuestion)\n",
    "\n",
    "        \n",
    "def cost_function(factsDf, questionFact, excludeColumns=[]):\n",
    "    cost = 0\n",
    "    maxFactIdx = 0\n",
    "    columnNames = [\"Subject\",\"Relation\", \"Objects\", \"States\", \"Times\", \"Locations\"]\n",
    "    for column in excludeColumns:\n",
    "        columnNames.remove(column)\n",
    "    for factIdx, factRow in factsDf.iterrows():\n",
    "        currCost = 0\n",
    "        for _, questionRow in questionFact.iterrows():\n",
    "            if len(factRow[excludeColumns[0]]) == 0:\n",
    "                continue\n",
    "            for column in columnNames:\n",
    "                currCost += similarity(factRow, questionRow, column)\n",
    "        if currCost > cost:\n",
    "            cost = currCost\n",
    "            maxFactIdx = factIdx\n",
    "    return maxFactIdx, cost\n",
    "\n",
    "\n",
    "def process_question_context(question, doc):\n",
    "    splitted_question = question.split(\" \")\n",
    "    question_type = splitted_question[0].lower()\n",
    "    question_nlp = nlp(question)\n",
    "    if question_nlp[0].ent_type_ == \"DATE\":\n",
    "        question_type = \"when\"\n",
    "    resolved_doc = preprocess_context(doc)\n",
    "    cleaned_doc = nlp(resolved_doc)\n",
    "    sentences = [one_sentence.text.strip() for one_sentence in cleaned_doc.sents]\n",
    "    \n",
    "    questionDF = extract_facts(question)\n",
    "    if len(questionDF) == 1:\n",
    "        questionDF[\"Subject\"] = question_nlp.text\n",
    "        questionDF[\"Relation\"] = question_nlp.text\n",
    "        questionDF[\"Objects\"] = question_nlp.text\n",
    "        questionDF[\"States\"] = question_nlp.text\n",
    "        questionDF[\"Times\"] = question_nlp.text\n",
    "        questionDF[\"Locations\"] = question_nlp.text\n",
    "    factsDF = join_sentences_facts(sentences)\n",
    "    \n",
    "    newFactsDF = change_subject_relation(factsDF, False)\n",
    "    newQuestionDF = change_subject_relation(questionDF, False)\n",
    "    \n",
    "    return newFactsDF, newQuestionDF, question_type\n",
    "\n",
    "def get_answer(factsDF, questionDF, question_type):\n",
    "    correctIdx, _ = cost_function(factsDF, questionDF, excludeColumns=[excludesPerQuestionType[question_type]])\n",
    "    answer = factsDF.loc[correctIdx, excludesPerQuestionType[question_type]]\n",
    "    if answer == []:\n",
    "        answer = factsDF.loc[correctIdx, \"States\"]    \n",
    "    return \" \".join(answer)\n",
    "    \n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "nlp.add_pipe(\"merge_noun_chunks\")\n",
    "nlp.add_pipe('coreferee')\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "excludesPerQuestionType = {\n",
    "    \"when\": \"Times\",\n",
    "    \"where\": \"Locations\",\n",
    "    \"who\": \"Subject\",\n",
    "    \"what\": \"Objects\",\n",
    "    \"how\": \"States\"\n",
    "}   \n",
    "    \n",
    "# doc = \"\"\"\n",
    "# Lionel Andrés \"Leo\" Messi was born in 24 June 1987 is an Argentine professional footballer plays as a forward for and captains both Major League Soccer club Inter Miami and the Argentina national team.\n",
    "# He played in Barcelona in 2010.\n",
    "# Widely regarded as one of the greatest players of all time, Messi has won a record eight Ballon d'Or awards, a record six European Golden Shoes, and was named the world's best player for a record eight times by FIFA.\n",
    "# Until 2021, he had spent his entire professional career with Barcelona, where he won a club-record 34 trophies, including ten La Liga titles, seven Copa del Rey titles, and the UEFA Champions League four times.\n",
    "# With his country, he won the 2021 Copa América and the 2022 FIFA World Cup. A prolific goalscorer and creative playmaker, Messi holds the records for most goals, hat-tricks, and assists in La Liga. He has the most international goals by a South American male. Messi has scored over 800 senior career goals for club and country, and the most goals for a single club.\n",
    "# \"\"\"\n",
    "# question = \"how did messi play?\"\n",
    "# factsDF, questionDF, question_type = process_question_context(question, doc)\n",
    "# answer = get_answer(factsDF, questionDF, question_type)\n",
    "\n",
    "# print(\"========================================================\")\n",
    "# print(\"Question: \", question)\n",
    "# print(\"Answer: \", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rajpurkar/squad\")\n",
    "# train = dataset['train']\n",
    "validation = dataset['validation']\n",
    "\n",
    "random.seed(42)\n",
    "import getopt\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import json \n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10570"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[260,\n",
       " 1420,\n",
       " 5063,\n",
       " 5788,\n",
       " 6600,\n",
       " 6808,\n",
       " 6885,\n",
       " 6904,\n",
       " 7362,\n",
       " 7459,\n",
       " 8962,\n",
       " 9035,\n",
       " 9599,\n",
       " 9667,\n",
       " 9877,\n",
       " 10253,\n",
       " 10403,\n",
       " 10422]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get length of questions which start with when and doesn't contain what, who, where, how and save the fake when questions indicies\n",
    "when_questions = []\n",
    "fake_when_questions = []\n",
    "fake_when_questions_indices = []\n",
    "for index, question in enumerate(validation['question']):\n",
    "    if question.startswith(\"When\") and not any(word in question.lower() for word in [\", what\", \", who\", \", where\", \", how\"]):\n",
    "        when_questions.append(question)\n",
    "    elif question.startswith(\"When\"):\n",
    "        fake_when_questions.append(question)\n",
    "        fake_when_questions_indices.append(index)\n",
    "        \n",
    "    \n",
    "        \n",
    "# remove rows from validation which contains questions inside fake_when_questions_indices\n",
    "validation = validation.filter(lambda example, idx: idx not in fake_when_questions_indices, with_indices=True)\n",
    "fake_when_questions_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10552"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1000/10552 [00:10<03:16, 48.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 11, out of 34: 32.35294117647059%\n",
      "EM: 1, out of 34: 2.9411764705882355%\n",
      "BLEU: 9.726965773204334, out of 34: 28.60872286236569%\n",
      "BLEU1: 14.396272864234573, out of 34: 42.34197901245462%\n",
      "BLEU2: 10.921672994256284, out of 34: 32.122567630165534%\n",
      "BLEU3: 9.487509362101699, out of 34: 27.904439300299117%\n",
      "BLEU4: 8.683629935592467, out of 34: 25.540088045860198%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1984/10552 [00:27<01:27, 98.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 33, out of 97: 34.02061855670103%\n",
      "EM: 17, out of 97: 17.52577319587629%\n",
      "BLEU: 32.11004231485293, out of 97: 33.10313640706488%\n",
      "BLEU1: 42.40285022997555, out of 97: 43.71427858760366%\n",
      "BLEU2: 33.87904057434809, out of 97: 34.92684595293617%\n",
      "BLEU3: 30.476275837764373, out of 97: 31.41884106986018%\n",
      "BLEU4: 28.715016302224946, out of 97: 29.603109589922624%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 3009/10552 [00:40<02:19, 54.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 47, out of 134: 35.07462686567164%\n",
      "EM: 21, out of 134: 15.671641791044776%\n",
      "BLEU: 43.60780555196087, out of 134: 32.54313847161259%\n",
      "BLEU1: 56.00398996791866, out of 134: 41.794022364118405%\n",
      "BLEU2: 44.7500200667397, out of 134: 33.39553736323858%\n",
      "BLEU3: 40.41391317289923, out of 134: 30.159636696193456%\n",
      "BLEU4: 38.14017524240127, out of 134: 28.462817345075575%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3990/10552 [00:52<00:40, 163.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 56, out of 164: 34.146341463414636%\n",
      "EM: 24, out of 164: 14.634146341463415%\n",
      "BLEU: 51.73249165763776, out of 164: 31.544202230266926%\n",
      "BLEU1: 67.27648996505964, out of 164: 41.0222499786949%\n",
      "BLEU2: 53.32943071000831, out of 164: 32.51794555488311%\n",
      "BLEU3: 47.97926380951687, out of 164: 29.255648664339553%\n",
      "BLEU4: 45.10963919704107, out of 164: 27.505877559171385%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4982/10552 [01:05<02:46, 33.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 64, out of 198: 32.323232323232325%\n",
      "EM: 29, out of 198: 14.646464646464647%\n",
      "BLEU: 60.14417567646138, out of 198: 30.375846301243122%\n",
      "BLEU1: 79.3959717057536, out of 198: 40.09897560896646%\n",
      "BLEU2: 62.344310071079086, out of 198: 31.487025288423784%\n",
      "BLEU3: 55.5789375985511, out of 198: 28.07017050431874%\n",
      "BLEU4: 51.905716543517265, out of 198: 26.21500835531175%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 5849/10552 [01:17<01:08, 68.21it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 71, out of 229: 31.004366812227076%\n",
      "EM: 34, out of 229: 14.847161572052402%\n",
      "BLEU: 69.21768330854937, out of 229: 30.22606258015256%\n",
      "BLEU1: 90.3795995536418, out of 229: 39.46707404089162%\n",
      "BLEU2: 71.47948388642934, out of 229: 31.213748422021542%\n",
      "BLEU3: 63.93194461710433, out of 229: 27.917879745460404%\n",
      "BLEU4: 59.738695705656546, out of 229: 26.086766683692815%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 6998/10552 [01:31<00:13, 259.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 80, out of 258: 31.007751937984494%\n",
      "EM: 35, out of 258: 13.565891472868216%\n",
      "BLEU: 74.68774888511221, out of 258: 28.94873987795047%\n",
      "BLEU1: 98.26541801939418, out of 258: 38.08737132534658%\n",
      "BLEU2: 76.6020468490663, out of 258: 29.690715832971435%\n",
      "BLEU3: 68.32207892410275, out of 258: 26.48142593957471%\n",
      "BLEU4: 63.76198443310685, out of 258: 24.71394745469258%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 7962/10552 [01:35<00:08, 293.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 85, out of 270: 31.48148148148148%\n",
      "EM: 37, out of 270: 13.703703703703704%\n",
      "BLEU: 78.02538250229046, out of 270: 28.898289815663134%\n",
      "BLEU1: 102.90113804138355, out of 270: 38.111532607919834%\n",
      "BLEU2: 80.19273511064534, out of 270: 29.70101300394272%\n",
      "BLEU3: 71.5428648738088, out of 270: 26.497357360669923%\n",
      "BLEU4: 66.84143136832775, out of 270: 24.75608569197324%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 8593/10552 [01:52<00:25, 77.05it/s] c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 82%|████████▏ | 8674/10552 [01:53<00:18, 104.24it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 83%|████████▎ | 8756/10552 [01:53<00:18, 97.42it/s] c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 85%|████████▍ | 8945/10552 [01:57<00:22, 70.26it/s] c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 85%|████████▍ | 8956/10552 [01:57<00:24, 66.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 93, out of 331: 28.09667673716012%\n",
      "EM: 40, out of 331: 12.084592145015106%\n",
      "BLEU: 89.70917598367197, out of 331: 27.102470085701505%\n",
      "BLEU1: 119.75748870511767, out of 331: 36.180510182815006%\n",
      "BLEU2: 90.66809508709726, out of 331: 27.392173742325458%\n",
      "BLEU3: 79.94651884921835, out of 331: 24.153026842664154%\n",
      "BLEU4: 74.11394286005749, out of 331: 22.39091929306873%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 9195/10552 [02:03<01:46, 12.72it/s] c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 87%|████████▋ | 9203/10552 [02:04<02:00, 11.19it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 89%|████████▉ | 9369/10552 [02:12<00:46, 25.71it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 92%|█████████▏| 9660/10552 [02:17<00:07, 120.41it/s]c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 92%|█████████▏| 9673/10552 [02:18<00:15, 57.31it/s] c:\\Users\\zisak\\anaconda3\\Lib\\site-packages\\coreferee\\tendencies.py:319: UserWarning: [W008] Evaluating Lexeme.similarity based on empty vectors.\n",
      "  referred_head_lexeme.similarity(referring_head_lexeme)\n",
      " 95%|█████████▍| 9997/10552 [02:26<00:13, 40.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 108, out of 400: 27.0%\n",
      "EM: 43, out of 400: 10.75%\n",
      "BLEU: 102.98285502952864, out of 400: 25.74571375738216%\n",
      "BLEU1: 141.21840620715292, out of 400: 35.30460155178823%\n",
      "BLEU2: 104.74844810908009, out of 400: 26.18711202727002%\n",
      "BLEU3: 91.03907405515244, out of 400: 22.759768513788107%\n",
      "BLEU4: 83.50820165622403, out of 400: 20.877050414056008%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10552/10552 [02:43<00:00, 64.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 114, out of 431: 26.45011600928074%\n",
      "EM: 44, out of 431: 10.208816705336426%\n",
      "BLEU: 110.38802863047235, out of 431: 25.612071607998224%\n",
      "BLEU1: 151.86970645919962, out of 431: 35.23659082580038%\n",
      "BLEU2: 113.03159572667822, out of 431: 26.225428242848775%\n",
      "BLEU3: 98.14737674219926, out of 431: 22.772013165243447%\n",
      "BLEU4: 90.05480145366859, out of 431: 20.89438548809016%\n",
      "Errors:  []\n",
      "Empties:  47 [1195, 1508, 1668, 1964, 2042, 2409, 2431, 2473, 3393, 3892, 3906, 4220, 4584, 4592, 4624, 4628, 4631, 5077, 5459, 5460, 6151, 6389, 6398, 6626, 7485, 7825, 8735, 8748, 8755, 8764, 8791, 8805, 8825, 8826, 8829, 8844, 8887, 8889, 8890, 8920, 8942, 8956, 9086, 9467, 9473, 9542, 10116]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def QuestionStartsWith_Accuracy(dataset, startsWith):\n",
    "\n",
    "    correct = 0\n",
    "    EM = 0\n",
    "    BLEU = 0\n",
    "    BLEU1 = 0\n",
    "    BLEU2 = 0\n",
    "    BLEU3 = 0\n",
    "    BLEU4 = 0\n",
    "    total = 0\n",
    "    errors = []\n",
    "    corrects = []\n",
    "    empties = []\n",
    "    kolo = 0\n",
    "    for item in tqdm(dataset):\n",
    "        random_number = random.randint(0, len(dataset))\n",
    "        kolo += 1\n",
    "        included_question = False\n",
    "        try:\n",
    "            context = item['context']\n",
    "            context = re.sub(' +', ' ', context)\n",
    "            \n",
    "            question = item['question']\n",
    "            tempQuestion = question.lower()\n",
    "            question = re.sub(' +', ' ', question)\n",
    "            \n",
    "            answer = item['answers']['text'][0]\n",
    "            title = item['title']\n",
    "    \n",
    "            # check if question starts with startsWith\n",
    "            for start in startsWith:\n",
    "                if tempQuestion.startswith(start):\n",
    "                    included_question = True\n",
    "                    break\n",
    "            \n",
    "            if included_question:\n",
    "    \n",
    "                total += 1\n",
    "                if total == 74:\n",
    "                    pass\n",
    "                    \n",
    "                factsDF, questionDF, question_type = process_question_context(question, context)\n",
    "                outputAnswer = get_answer(factsDF, questionDF, question_type)\n",
    "                if outputAnswer == \"\":\n",
    "                    empties.append(kolo)\n",
    "                    outputAnswer = \"No_Answer_Found\"\n",
    "                \n",
    "                # print(\"Question: \" , question)\n",
    "                # print(\"Answer: \", answer, \"-------\" , \"Our Answer: \", outputAnswer)\n",
    "                if outputAnswer in answer or answer in outputAnswer:\n",
    "                    correct += 1\n",
    "                    corrects.append(kolo)\n",
    "                else:\n",
    "                    # write to when_wrong_answers.txt\n",
    "                    with open(\"results/when_wrong_answers.txt\", \"a\") as file:\n",
    "                        file.write(f\"Question: {question}\\n\")\n",
    "                        file.write(f\"Answer: {answer}\\n\")\n",
    "                        file.write(f\"Our Answer: {outputAnswer}\\n\\n\")\n",
    "                        \n",
    "                        \n",
    "                if outputAnswer == answer:\n",
    "                    EM += 1\n",
    "\n",
    "                n = min(len(outputAnswer.split()), 4)\n",
    "                if n == 0:\n",
    "                    BLEUscore = 0\n",
    "                else:\n",
    "                    weights = [1.0/n]*n\n",
    "                    smoothie = SmoothingFunction().method4\n",
    "                    BLEUscore = nltk.translate.bleu_score.sentence_bleu([answer], outputAnswer, weights=weights, smoothing_function=smoothie)\n",
    "                    \n",
    "                    BLEU += BLEUscore\n",
    "                    BLEU1 += nltk.translate.bleu_score.sentence_bleu([answer], outputAnswer, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "                    BLEU2 += nltk.translate.bleu_score.sentence_bleu([answer], outputAnswer, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "                    BLEU3 += nltk.translate.bleu_score.sentence_bleu([answer], outputAnswer, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)\n",
    "                    BLEU4 += nltk.translate.bleu_score.sentence_bleu([answer], outputAnswer, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "                    \n",
    "\n",
    "            if kolo % 1000 == 0:\n",
    "                print(f\"Correct: {correct}, out of {total}: {100*correct/total}%\")\n",
    "                print(f\"EM: {EM}, out of {total}: {100*EM/total}%\")\n",
    "                print(f\"BLEU: {BLEU}, out of {total}: {100*BLEU/total}%\")\n",
    "                print(f\"BLEU1: {BLEU1}, out of {total}: {100*BLEU1/total}%\")\n",
    "                print(f\"BLEU2: {BLEU2}, out of {total}: {100*BLEU2/total}%\")\n",
    "                print(f\"BLEU3: {BLEU3}, out of {total}: {100*BLEU3/total}%\")\n",
    "                print(f\"BLEU4: {BLEU4}, out of {total}: {100*BLEU4/total}%\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"title: \", title)\n",
    "            print(\"Error in question number: \", total)\n",
    "            print(\"Question: \", question)\n",
    "            print(\"Answer: \", answer)\n",
    "            print(\"Error: \", e)\n",
    "            errors.append(total)\n",
    "            # total -= 1\n",
    "            print(\"\\n\\n\")\n",
    "    \n",
    "    if total != 0:\n",
    "        print(f\"Correct: {correct}, out of {total}: {100*correct/total}%\")\n",
    "        print(f\"EM: {EM}, out of {total}: {100*EM/total}%\")\n",
    "        print(f\"BLEU: {BLEU}, out of {total}: {100*BLEU/total}%\")\n",
    "        print(f\"BLEU1: {BLEU1}, out of {total}: {100*BLEU1/total}%\")\n",
    "        print(f\"BLEU2: {BLEU2}, out of {total}: {100*BLEU2/total}%\")\n",
    "        print(f\"BLEU3: {BLEU3}, out of {total}: {100*BLEU3/total}%\")\n",
    "        print(f\"BLEU4: {BLEU4}, out of {total}: {100*BLEU4/total}%\")\n",
    "    else:\n",
    "        print(\"No Questions found with the given starting word\")\n",
    "    print(\"Errors: \", errors)\n",
    "    print(\"Empties: \", len(empties), empties)\n",
    "    return correct, total, errors, corrects\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    startsWith = [\"where \"]\n",
    "    x = QuestionStartsWith_Accuracy(validation, startsWith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [174, 315, 326, 349, 472, 488, 493, 498, 530, 667]\n",
    "# when: 48%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
